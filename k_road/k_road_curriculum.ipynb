{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 14:09:29.226424 139987772512064 deprecation.py:323] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/pyarrow_files', '/home/ctripp/src/cavs-environments/cavs_environments/vehicle/k_road', '/home/ctripp/src/cavs-environments/cavs_environments/vehicle/k_road', '/home/ctripp/src/cavs-environments/cavs_environments/vehicle/k_road/~/project/flow/flow', '/home/ctripp/miniconda3/envs/cavs-environments/lib/python37.zip', '/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7', '/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/lib-dynload', '', '/home/ctripp/.local/lib/python3.7/site-packages', '/home/ctripp/.local/lib/python3.7/site-packages/carla-0.9.5-py3.5-linux-x86_64.egg', '/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages', '/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/carla-0.9.5-py3.7-linux-x86_64.egg', '/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/IPython/extensions', '/home/ctripp/.ipython', '/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/modin', '../../../', '/home/ctripp/src/cavs-environments']\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Loading chipmunk for Linux (64bit) [/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/pymunk/libchipmunk.so]\n",
      "Registered cavs_environments!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 14:09:30,254\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-30_14-09-30_253880_20837/logs.\n",
      "2019-07-30 14:09:30,367\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:31314 to respond...\n",
      "2019-07-30 14:09:30,491\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:18472 to respond...\n",
      "2019-07-30 14:09:30,498\tINFO services.py:806 -- Starting Redis shard with 6.71 GB max memory.\n",
      "2019-07-30 14:09:30,549\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-30_14-09-30_253880_20837/logs.\n",
      "2019-07-30 14:09:30,552\tINFO services.py:1446 -- Starting the Plasma object store with 10.07 GB memory using /dev/shm.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "        \n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.models import FullyConnectedNetwork, Model, ModelCatalog\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "sys.path.append(\"/home/ctripp/src/cavs-environments\")\n",
    "print(sys.path)\n",
    "\n",
    "import pygame\n",
    "import pymunk\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    " \n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import cavs_environments\n",
    "import matplotlib\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import TRPO\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy\n",
    "# from cavs_environments.vehicle.deep_road.deep_road import DeepRoad\n",
    "\n",
    "import cavs_environments.framework as framework\n",
    "import cavs_environments.vehicle.k_road.targeting as targeting\n",
    "import cavs_environments.vehicle.k_road.road as road\n",
    "# import  cavs_environments.vehicle.k_road.targeting as\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.init()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# ModelCatalog.register_custom_model(\"ThisRoadEnv\", ThisRoadEnv)\n",
    "# register_env(\"ThisRoadEnv\", lambda config: ThisRoadEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.display.quit()\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def make_target_env_with_baseline(\n",
    "#     observation_scaling = 1.0, \n",
    "#     action_scaling = 1.0 / 10.0,\n",
    "#     max_distance_from_target = 125, \n",
    "#     time_limit = 60):\n",
    "    \n",
    "#     return framework.FactoredGym(\n",
    "#         targeting.TargetProcess(time_limit, max_distance_from_target),\n",
    "#         targeting.TargetObserver(observation_scaling),\n",
    "#         targeting.TargetTerminator(),\n",
    "#         targeting.TargetRewarder(),\n",
    "#         [framework.ActionScaler(action_scaling), targeting.TargetBaseline()]\n",
    "#         )\n",
    "\n",
    "class ThisRoadEnv(framework.FactoredGym):\n",
    "    def __init__(self, env_config):\n",
    "        observation_scaling = 1.0\n",
    "        action_scaling = 1.0 / 10.0\n",
    "        super().__init__(\n",
    "        road.RoadProcess(),\n",
    "        road.RoadObserver(observation_scaling),\n",
    "        road.RoadTerminator(),\n",
    "        road.RoadGoalRewarder(),\n",
    "        [framework.ActionScaler(action_scaling)]\n",
    "        )\n",
    "\n",
    "def make_road_env(\n",
    "    observation_scaling = 1.0,\n",
    "    action_scaling = 1.0 / 10.0):\n",
    "    return ThisRoadEnv(observation_scaling, action_scaling)\n",
    "\n",
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(*args, **kwargs,\n",
    "                                           net_arch=[64, 64, dict(pi=[64],\n",
    "                                                          vf=[64])],\n",
    "                                           feature_extraction=\"mlp\")\n",
    "\n",
    "# class CustomPolicy(MlpPolicy):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(MlpPolicy, self).__init__(*args, act_fun=tf.nn.tanh, net_arch=[32, 32])\n",
    "        \n",
    "# register_policy('LargeMLP', LargeMLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 14:09:40,516\tINFO tune.py:61 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
      "2019-07-30 14:09:40,517\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-30 14:09:40,532\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "W0730 14:09:40.540739 139987772512064 deprecation_wrapper.py:119] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/tune/logger.py:136: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.\n",
      "\n",
      "W0730 14:09:40.541399 139987772512064 deprecation_wrapper.py:119] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/tune/logger.py:141: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/12 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/33.6 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/12 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/33.6 GB\n",
      "Result logdir: /home/ctripp/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_ThisRoadEnv_0:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 14:09:41,602\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 436, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 323, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/worker.py\", line 2195, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_PPO:train()\u001b[39m (pid=20926, host=nrel-34256s)\n",
      "  File \"pyarrow/serialization.pxi\", line 461, in pyarrow.lib.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 424, in pyarrow.lib.deserialize_from\n",
      "  File \"pyarrow/serialization.pxi\", line 275, in pyarrow.lib.SerializedPyObject.deserialize\n",
      "  File \"pyarrow/serialization.pxi\", line 174, in pyarrow.lib.SerializationContext._deserialize_callback\n",
      "ModuleNotFoundError: No module named 'cavs_environments'\n",
      "\n",
      "2019-07-30 14:09:41,604\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_ThisRoadEnv_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20926)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=20926)\u001b[0m W0730 14:09:41.435480 139781480658752 deprecation.py:323] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=20926)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=20926)\u001b[0m non-resource variables are not supported in the long term\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/12 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/33.6 GB\n",
      "Result logdir: /home/ctripp/ray_results/PPO\n",
      "Number of trials: 1 ({'ERROR': 1})\n",
      "ERROR trials:\n",
      " - PPO_ThisRoadEnv_0:\tERROR, 1 failures: /home/ctripp/ray_results/PPO/PPO_ThisRoadEnv_0_2019-07-30_14-09-40t3jmnxkf/error_2019-07-30_14-09-41.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_ThisRoadEnv_0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fbc0bc8147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;34m\"fcnet_hiddens\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         },\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;34m\"num_workers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     })\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_function, checkpoint_freq, checkpoint_at_end, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_ThisRoadEnv_0])"
     ]
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"timesteps_total\": 10000\n",
    "    },\n",
    "    config={\n",
    "        \"env\": ThisRoadEnv,\n",
    "        \"model\":{\n",
    "            \"fcnet_hiddens\":[64,64]\n",
    "        },\n",
    "        \"num_workers\": 1\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 14:12:56,818\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "2019-07-30 14:12:56,853\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "W0730 14:12:56.884514 139987772512064 deprecation.py:323] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0730 14:12:57.081722 139987772512064 deprecation.py:323] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2019-07-30 14:12:57,150\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\n",
      "{ 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "  'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "  'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "  'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "  'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "  'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 204) dtype=float32>,\n",
      "  'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 204) dtype=float32>,\n",
      "  'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "  'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "  'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "  'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "  'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\n",
      "W0730 14:12:57.188670 139987772512064 deprecation.py:323] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2019-07-30 14:12:57,811\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f511a196ef0>}\n",
      "2019-07-30 14:12:57,811\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f511a196a20>}\n",
      "2019-07-30 14:12:57,811\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f511a1962b0>}\n",
      "2019-07-30 14:12:57,835\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20919)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=20919)\u001b[0m W0730 14:12:58.772580 140541510301504 deprecation.py:323] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=20919)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=20919)\u001b[0m non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0730 14:12:59.636049 139987772512064 deprecation.py:323] From /home/ctripp/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "2019-07-30 14:12:59,669\tINFO ppo.py:106 -- Important! Since 0.7.0, observation normalization is no longer enabled by default. To enable running-mean normalization, set 'observation_filter': 'MeanStdFilter'. You can ignore this message if your environment doesn't require observation normalization.\n",
      "2019-07-30 14:13:00,046\tINFO trainer.py:361 -- Worker crashed during call to train(). To attempt to continue training without the failed worker, set `'ignore_worker_failures': True`.\n"
     ]
    },
    {
     "ename": "RayTaskError",
     "evalue": "\u001b[36mray_RolloutWorker:sample()\u001b[39m (pid=20919, host=nrel-34256s)\n  File \"pyarrow/serialization.pxi\", line 461, in pyarrow.lib.deserialize\n  File \"pyarrow/serialization.pxi\", line 424, in pyarrow.lib.deserialize_from\n  File \"pyarrow/serialization.pxi\", line 275, in pyarrow.lib.SerializedPyObject.deserialize\n  File \"pyarrow/serialization.pxi\", line 174, in pyarrow.lib.SerializationContext._deserialize_callback\nModuleNotFoundError: No module named 'cavs_environments'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-64694b220af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m    \u001b[0;31m# Perform one iteration of training the policy with PPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m    \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m                         \u001b[0;34m\"continue training without the failed worker, set \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                         \"`'ignore_worker_failures': True`.\")\n\u001b[0;32m--> 364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow logs messages to propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_WORKER_FAILURE_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_worker_failures\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_train() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mafter_optimizer_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mafter_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     samples = collect_samples(\n\u001b[1;32m    129\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                         self.num_envs_per_worker, self.train_batch_size)\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     logger.info(\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/optimizers/rollout.py\u001b[0m in \u001b[0;36mcollect_samples\u001b[0;34m(agents, sample_batch_size, num_envs_per_worker, train_batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mfut_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnext_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_get_and_free\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnext_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msample_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_envs_per_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnum_timesteps_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/rllib/utils/memory.py\u001b[0m in \u001b[0;36mray_get_and_free\u001b[0;34m(object_ids)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_to_free\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mobject_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cavs-environments/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids)\u001b[0m\n\u001b[1;32m   2193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                 \u001b[0mlast_task_error_raise_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0;31m# Run post processors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError\u001b[0m: \u001b[36mray_RolloutWorker:sample()\u001b[39m (pid=20919, host=nrel-34256s)\n  File \"pyarrow/serialization.pxi\", line 461, in pyarrow.lib.deserialize\n  File \"pyarrow/serialization.pxi\", line 424, in pyarrow.lib.deserialize_from\n  File \"pyarrow/serialization.pxi\", line 275, in pyarrow.lib.SerializedPyObject.deserialize\n  File \"pyarrow/serialization.pxi\", line 174, in pyarrow.lib.SerializationContext._deserialize_callback\nModuleNotFoundError: No module named 'cavs_environments'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# env = make_road_env()\n",
    "\n",
    "# config = ppo.DEFAULT_CONFIG.copy()\n",
    "# config.\n",
    "config = config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "}\n",
    "config[\"num_gpus\"] = 1\n",
    "config[\"num_workers\"] = 1\n",
    "# config['use_eager'] = True\n",
    "# config['stop'] = 10e3\n",
    "# config['model'] = {\"custom_model\":\"my_model\"}\n",
    "trainer = ppo.PPOTrainer(config=config, env=ThisRoadEnv)\n",
    "\n",
    "for i in range(1000):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "   result = trainer.train()\n",
    "   print(pretty_print(result))\n",
    "\n",
    "   if i % 100 == 0:\n",
    "       checkpoint = trainer.save()\n",
    "       print(\"checkpoint saved at\", checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ego_starting_distance = 10.0\n",
    "target_reward = 90\n",
    "iter = 0\n",
    "step = 1e3\n",
    "\n",
    "inner_env = make_road_env()\n",
    "inner_env.process.ego_starting_distance = ego_starting_distance\n",
    "inner_env.reset()\n",
    "\n",
    "env = DummyVecEnv([lambda: inner_env])  # The algorithms require a vectorized environment to run\n",
    "model = PPO2(CustomPolicy, env, verbose=0, tensorboard_log='/tmp/k_road_0/',\n",
    "            gamma=.999, learning_rate=.001)\n",
    "\n",
    "while iter < 250e3:\n",
    "    \n",
    "    inner_env.process.ego_starting_distance  = ego_starting_distance\n",
    "    model.learn(total_timesteps=int(step), reset_num_timesteps = False)\n",
    "    \n",
    "    print('er: ', model.episode_reward)\n",
    "    mean_reward = np.mean(model.episode_reward)\n",
    "    \n",
    "    if mean_reward >= target_reward:\n",
    "        ego_starting_distance = min(600.0, ego_starting_distance + 10.0)\n",
    "        \n",
    "    print('iter: ', iter, 'reward: ', mean_reward, 'starting_distance: ', inner_env.process.ego_starting_distance)\n",
    "    \n",
    "    iter = iter + step\n",
    "model.save('k_road_0')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_road_env()\n",
    "env.process.ego_starting_distance = 10\n",
    "\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "model = PPO2.load('k_road_0')\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    for i in range(100000):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, terminal, info = env.step(action)\n",
    "        env.render()\n",
    "        if terminal:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_road_env()\n",
    "env.ego_starting_distance = 10\n",
    "print(env.process.ego_vehicle, env.process.road_length, env.process.ego_starting_distance)\n",
    "\n",
    "obs = None\n",
    "for _ in range(20):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for _ in range(2000):\n",
    "        action = np.empty(2)\n",
    "        action[0] = .5 # np.random.normal(.5, .001)\n",
    "        action[1] = 0 # np.random.normal(0, .01)\n",
    "\n",
    "        result = env.step(action)\n",
    "        obs = result[0]\n",
    "        env.render()\n",
    "        if result[2]:\n",
    "            break\n",
    "       \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_cpu = 10\n",
    "env = SubprocVecEnv([lambda: make_road_env() for i in range(n_cpu)])\n",
    "model = PPO2(MlpPolicy, env, verbose=1, tensorboard_log='/tmp/k_road_0/',\n",
    "            gamma=.99999, learning_rate=.001)\n",
    "model.learn(total_timesteps=int(200e3))\n",
    "model.save('k_road_0')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_road_env()\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "model = PPO2.load('k_road_0')\n",
    "obs = env.reset()\n",
    "for i in range(100000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, terminal, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminal:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "def two_sided_offset_exponential(gain, offset, x):\n",
    "    y = 0\n",
    "    s = 1\n",
    "    if x < offset:\n",
    "        y = 1 - (x + 1) / (offset + 1)\n",
    "        s = -1\n",
    "    else:\n",
    "        y = (x - offset) / (1 - offset)\n",
    "    return s * (math.exp(gain * y) - 1) / (math.exp(gain) - 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "x = np.arange(-1.0, 1.0, .01)\n",
    "y = np.array([two_sided_offset_exponential(.1, -.5, xi) for xi in x])\n",
    "plt.plot(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "def train(config, reporter):\n",
    "    trainer = PPOTrainer(config=config, env=YourEnv)\n",
    "    while True:\n",
    "        result = trainer.train()\n",
    "        reporter(**result)\n",
    "        if result[\"episode_reward_mean\"] > 200:\n",
    "            phase = 2\n",
    "        elif result[\"episode_reward_mean\"] > 100:\n",
    "            phase = 1\n",
    "        else:\n",
    "            phase = 0\n",
    "        trainer.workers.foreach_worker(\n",
    "            lambda ev: ev.foreach_env(\n",
    "                lambda env: env.set_phase(phase)))\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    train,\n",
    "    config={\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 2,\n",
    "    },\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": lambda spec: spec.config.num_gpus,\n",
    "        \"extra_cpu\": lambda spec: spec.config.num_workers,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cavs-environments] *",
   "language": "python",
   "name": "conda-env-cavs-environments-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
